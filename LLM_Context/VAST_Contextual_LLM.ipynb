{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDIagBmt6cR4"
      },
      "source": [
        "**Accessing via Huggingface API**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jjIzEc2cW7PT",
        "outputId": "32dff3f9-a414-45d9-c75d-7d71a127b84c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.27.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wABJtoQ1Hmb4",
        "outputId": "84bd0794-aec5-493d-8e70-3b30682577a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install pandas requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYVsK0jDHbFD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "\n",
        "# -------------------------------------------------\n",
        "# CONFIGURATION\n",
        "# -------------------------------------------------\n",
        "HF_API_KEY = \"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\"\n",
        "MODEL_ID = \"NousResearch/Hermes-3-Llama-3.1-8B\"\n",
        "\n",
        "# CSV splits to process\n",
        "csv_files = [\"/content/VAST_train.csv\"]\n",
        "\n",
        "# If your dataset has a \"dataset_description\"\n",
        "# you might define it here:\n",
        "DATASET_DESCRIPTION = \"VAST isimli, 2020 yılında oluşturulmuş bu dataset, The New York Times’in ‘Room for Debate’ bölümündeki yorumların etiketlenmesiyle hazırlanmış olup, özellikle tutum (stance) tespiti görevleri için tasarlanmıştır. Politika (örneğin, 'Filistin devleti'), eğitim (örneğin, 'imtiyazlı okullar') ve halk sağlığı (örneğin, 'çocukluk aşıları') gibi geniş temaları kapsayan çeşitli konuları kapsar. Ayrıca, ‘kampüste silahlar’ ile ‘kampüste ateşli silahlar’ gibi benzer ifadelerin bir arada bulunduğu örnekleri de içerir.\"\n",
        "\n",
        "# -------------------------------------------------\n",
        "# HELPER: Generate 'Context' text from model\n",
        "# -------------------------------------------------\n",
        "def generate_context(text, target):\n",
        "    \"\"\"\n",
        "    Calls the Hugging Face Inference API with the prompt\n",
        "    and returns the model's response as a string.\n",
        "    \"\"\"\n",
        "    # Build prompt\n",
        "    prompt = f\"\"\"Sosyal bir uzman olduğunu varsayarak, aşağıda {DATASET_DESCRIPTION} datasetinin kısa bir pasajı verilmiştir, lütfen adım adım düşün, metindeki anahtar kelimeleri çıkar, yazarın ima ettiği duyguları, retorik araçları vb. analiz et, son olarak yazarın Target hakkındaki duruşunu kısaca analiz et, sonuca varmadan analiz sürecini vermeye dikkat et. Ayrıca açıklama 100 kelimeyi geçmesin ve hiçbir şekilde link içermesin.\n",
        "Passage: {text}\n",
        "Target: {target}\n",
        "\"\"\"\n",
        "\n",
        "    # Prepare JSON payload for the HF Inference API\n",
        "    payload = {\n",
        "        \"inputs\": prompt,\n",
        "        \"parameters\": {\n",
        "            \"max_new_tokens\": 500,\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Make request\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {HF_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    HF_API_URL = f\"https://api-inference.huggingface.co/models/{MODEL_ID}\"\n",
        "\n",
        "    response = requests.post(HF_API_URL, headers=headers, json=payload)\n",
        "\n",
        "    # If no success, handle errors\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Error: {response.status_code}, {response.text}\")\n",
        "        return \"\"\n",
        "\n",
        "    data = response.json()\n",
        "    # data is typically [{\"generated_text\": \"...\"}]\n",
        "    if isinstance(data, list) and len(data) > 0 and \"generated_text\" in data[0]:\n",
        "        return data[0][\"generated_text\"]\n",
        "    elif isinstance(data, dict) and \"generated_text\" in data:\n",
        "        return data[\"generated_text\"]\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "# -------------------------------------------------\n",
        "# MAIN: For each CSV, add a \"Context\" column\n",
        "# -------------------------------------------------\n",
        "for csv_file in csv_files:\n",
        "    print(f\"Processing: {csv_file}\")\n",
        "    df = pd.read_csv(csv_file, encoding=\"utf-8\")\n",
        "\n",
        "    # create a new column \"Context\" by calling generate_context row by row.\n",
        "    context_list = []\n",
        "    for i, row in df.iterrows():\n",
        "        target_val = row[\"Target\"]\n",
        "        text_val = row[\"Text\"]\n",
        "\n",
        "        # Generate context from LLM\n",
        "        context_text = generate_context(text_val, target_val)\n",
        "\n",
        "        # For large datasets, you might want to add a short sleep\n",
        "        # to avoid rate limits or slow down\n",
        "        # time.sleep(1)\n",
        "\n",
        "        context_list.append(context_text)\n",
        "\n",
        "    # Add the new column\n",
        "    df[\"Context\"] = context_list\n",
        "\n",
        "    # Save the updated CSV\n",
        "    output_name = csv_file.replace(\".csv\", \"_with_context.csv\")\n",
        "    df.to_csv(output_name, index=False, encoding=\"utf-8-sig\")\n",
        "    print(f\"Saved: {output_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykXJxjKc6hwn"
      },
      "source": [
        "**Accesiing Locally**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "h0S6ouuo6o47",
        "outputId": "7c16c807-ce67-431a-d890-57ddb7204632"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchNote: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "  Using cached torch-2.5.1-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-1.2.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting sentencepiece\n",
            "  Using cached sentencepiece-0.2.0-cp312-cp312-win_amd64.whl.metadata (8.3 kB)\n",
            "Collecting filelock (from torch)\n",
            "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting typing-extensions>=4.8.0 (from torch)\n",
            "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting networkx (from torch)\n",
            "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jinja2 (from torch)\n",
            "  Downloading jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting fsspec (from torch)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting setuptools (from torch)\n",
            "  Using cached setuptools-75.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting sympy==1.13.1 (from torch)\n",
            "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
            "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n",
            "  Downloading huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting numpy>=1.17 (from transformers)\n",
            "  Using cached numpy-2.2.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\nuref\\desktop\\stance_detection_datasets\\.venv\\lib\\site-packages (from transformers) (24.2)\n",
            "Collecting pyyaml>=5.1 (from transformers)\n",
            "  Using cached PyYAML-6.0.2-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
            "Collecting regex!=2019.12.17 (from transformers)\n",
            "  Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
            "Collecting requests (from transformers)\n",
            "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
            "  Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
            "Collecting safetensors>=0.4.1 (from transformers)\n",
            "  Downloading safetensors-0.5.0-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
            "Collecting tqdm>=4.27 (from transformers)\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Requirement already satisfied: psutil in c:\\users\\nuref\\desktop\\stance_detection_datasets\\.venv\\lib\\site-packages (from accelerate) (6.1.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\nuref\\desktop\\stance_detection_datasets\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
            "  Using cached MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl.metadata (4.1 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests->transformers)\n",
            "  Downloading charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl.metadata (36 kB)\n",
            "Collecting idna<4,>=2.5 (from requests->transformers)\n",
            "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
            "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
            "  Downloading certifi-2024.12.14-py3-none-any.whl.metadata (2.3 kB)\n",
            "Using cached torch-2.5.1-cp312-cp312-win_amd64.whl (203.0 MB)\n",
            "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "Using cached transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
            "Downloading accelerate-1.2.1-py3-none-any.whl (336 kB)\n",
            "Using cached sentencepiece-0.2.0-cp312-cp312-win_amd64.whl (991 kB)\n",
            "Downloading huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n",
            "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "Using cached numpy-2.2.1-cp312-cp312-win_amd64.whl (12.6 MB)\n",
            "Using cached PyYAML-6.0.2-cp312-cp312-win_amd64.whl (156 kB)\n",
            "Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl (273 kB)\n",
            "Downloading safetensors-0.5.0-cp38-abi3-win_amd64.whl (303 kB)\n",
            "Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
            "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
            "Downloading jinja2-3.1.5-py3-none-any.whl (134 kB)\n",
            "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Using cached setuptools-75.6.0-py3-none-any.whl (1.2 MB)\n",
            "Downloading certifi-2024.12.14-py3-none-any.whl (164 kB)\n",
            "Downloading charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl (102 kB)\n",
            "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
            "Using cached MarkupSafe-3.0.2-cp312-cp312-win_amd64.whl (15 kB)\n",
            "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
            "Installing collected packages: sentencepiece, mpmath, urllib3, typing-extensions, tqdm, sympy, setuptools, safetensors, regex, pyyaml, numpy, networkx, MarkupSafe, idna, fsspec, filelock, charset-normalizer, certifi, requests, jinja2, torch, huggingface-hub, tokenizers, accelerate, transformers\n",
            "Successfully installed MarkupSafe-3.0.2 accelerate-1.2.1 certifi-2024.12.14 charset-normalizer-3.4.1 filelock-3.16.1 fsspec-2024.12.0 huggingface-hub-0.27.0 idna-3.10 jinja2-3.1.5 mpmath-1.3.0 networkx-3.4.2 numpy-2.2.1 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.3 safetensors-0.5.0 sentencepiece-0.2.0 setuptools-75.6.0 sympy-1.13.1 tokenizers-0.21.0 torch-2.5.1 tqdm-4.67.1 transformers-4.47.1 typing-extensions-4.12.2 urllib3-2.3.0\n"
          ]
        }
      ],
      "source": [
        "pip install torch transformers accelerate sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kDJ0PL6i9yjn",
        "outputId": "0e642fc7-8387-48c4-f97e-281bec3fdc18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated Git hooks.\n",
            "Git LFS initialized.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning into 'Hermes-3-Llama-3.1-8B'...\n",
            "Filtering content:  50% (2/4)\n",
            "Filtering content:  50% (2/4), 1.66 GiB | 1.44 MiB/s\n",
            "Filtering content:  75% (3/4), 1.66 GiB | 1.44 MiB/s\n",
            "Filtering content:  75% (3/4), 2.32 GiB | 2.98 MiB/s\n",
            "Filtering content: 100% (4/4), 2.32 GiB | 2.98 MiB/s\n",
            "Filtering content: 100% (4/4), 2.95 GiB | 4.38 MiB/s\n",
            "Filtering content: 100% (4/4), 2.95 GiB | 4.90 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git lfs install\n",
        "!git clone https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pandas\n",
            "  Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\nuref\\desktop\\stance_detection_datasets\\.venv\\lib\\site-packages (from pandas) (2.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nuref\\desktop\\stance_detection_datasets\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Collecting pytz>=2020.1 (from pandas)\n",
            "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas)\n",
            "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\nuref\\desktop\\stance_detection_datasets\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Using cached pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
            "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
            "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
            "Installing collected packages: pytz, tzdata, pandas\n",
            "Successfully installed pandas-2.2.3 pytz-2024.2 tzdata-2024.2\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "0a85adfed4df4ccba0c03d6b73731b2c",
            "9b3f1bed13034a3cba856f329ad22d9f",
            "b03468753cff49ec8ed41a092ff15611",
            "80ded8de1c774168b9d2c40d4797af28",
            "e83735d01e47463f8ed60f6f68bfaf4b",
            "a24c613e62f2495eb45a11c2b61a92f7",
            "4ea4c6c289be4f308c18d337f7126ebd",
            "561785e53e41408596ebd8a82a23d05a",
            "18c3b7d9d31c47f19f5015a5e3beb30d",
            "6495ce6380474d2eaabbee6df2586e7a",
            "147284d33327479ba769c224012f5bde"
          ]
        },
        "id": "IupXjGk--65O",
        "outputId": "49bba5ca-c0eb-4e81-e8fb-2c9288ffc54c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "############################\n",
        "# CONFIGURATION\n",
        "############################\n",
        "MODEL_PATH = \"C:/Users/nuref/Desktop/Stance_Detection_Datasets/LLM_Context/Hermes-3-Llama-3.1-8B\"  # local folder with the model\n",
        "INPUT_CSV = \"C:/Users/nuref/Desktop/Stance_Detection_Datasets/VAST/VAST_dev.csv\"                  \n",
        "OUTPUT_CSV = \"C:/Users/nuref/Desktop/Stance_Detection_Datasets/VAST/VAST_dev_with_context.csv\"     # output CSV with new 'Context' column\n",
        "\n",
        "DATASET_DESCRIPTION = (\n",
        "    \"VAST isimli, 2020 yılında oluşturulmuş bu dataset, The New York Times’in ‘Room for Debate’ bölümündeki yorumların \"\n",
        "    \"etiketlenmesiyle hazırlanmış olup, özellikle tutum (stance) tespiti görevleri için tasarlanmıştır. Politika (örneğin, 'Filistin devleti'),\"\n",
        "    \"eğitim (örneğin, 'imtiyazlı okullar') ve halk sağlığı (örneğin, 'çocukluk aşıları') gibi geniş temaları kapsayan çeşitli konuları kapsar. \"\n",
        "    \"Ayrıca, ‘kampüste silahlar’ ile ‘kampüste ateşli silahlar’ gibi benzer ifadelerin bir arada bulunduğu örnekleri de içerir.\"\n",
        ")\n",
        "\n",
        "############################\n",
        "# LOAD MODEL LOCALLY\n",
        "############################\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "print(\"Loading model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_PATH,\n",
        "    torch_dtype=torch.float16,  # or bfloat16, etc. adjust as needed\n",
        "    device_map=\"auto\",         # automatically map to GPU or CPU\n",
        ")\n",
        "\n",
        "############################\n",
        "# GENERATION FUNCTION\n",
        "############################\n",
        "def generate_context(text, target):\n",
        "    \"\"\"\n",
        "    Generates context from the local LLaMA model:\n",
        "    - No external links\n",
        "    - Under ~100 words\n",
        "    \"\"\"\n",
        "    # Prompt: no links, under 100 words\n",
        "    prompt = f\"\"\"\n",
        "Sosyal bir uzman olduğunu varsayarak, aşağıda {DATASET_DESCRIPTION} datasetinden\n",
        "kısa bir pasaj verilmiştir. Lütfen adım adım düşün, metindeki anahtar kelimeleri çıkar;\n",
        "yazarın ima ettiği duyguları, retorik araçları vb. analiz et;\n",
        "ve son olarak yazarın Target hakkındaki duruşuna dair kısa bir analiz yap, sonuca varmadan analiz sürecini vermeye dikkat et.\n",
        "Ayrıca açıklama 100 kelimeyi geçmesin ve hiçbir şekilde link içermesin.\n",
        "\n",
        "Passage: {text}\n",
        "Target: {target}\n",
        "\"\"\"\n",
        "\n",
        "    # Encode prompt\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generate text\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=512,        # limit generation\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "    # Decode the entire output\n",
        "    full_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Separate out the part after the prompt\n",
        "    prompt_len = len(tokenizer.encode(prompt))\n",
        "    response = tokenizer.decode(output_ids[0][prompt_len:], skip_special_tokens=True)\n",
        "\n",
        "    # Optional: a quick post-process to ensure we don't exceed ~100 words\n",
        "    #   (model might still exceed if it doesn't follow instructions)\n",
        "    response_words = response.strip().split()\n",
        "    if len(response_words) > 100:\n",
        "        response = \" \".join(response_words[:100])\n",
        "\n",
        "    return response.strip()\n",
        "\n",
        "############################\n",
        "# MAIN FLOW\n",
        "############################\n",
        "def main():\n",
        "    print(f\"Reading input CSV: {INPUT_CSV}\")\n",
        "    df = pd.read_csv(INPUT_CSV, encoding=\"utf-8\")\n",
        "\n",
        "    # We'll assume columns \"Text\" and \"Target\" exist in the CSV\n",
        "    # If your file uses different names, update accordingly.\n",
        "    if \"Text\" not in df.columns or \"Target\" not in df.columns:\n",
        "        raise ValueError(\"Input CSV must have 'Text' and 'Target' columns.\")\n",
        "\n",
        "    # Generate 'Context' for each row\n",
        "    contexts = []\n",
        "    for i, row in df.iterrows():\n",
        "        text_val = str(row[\"Text\"])\n",
        "        target_val = str(row[\"Target\"])\n",
        "        ctx = generate_context(text_val, target_val)\n",
        "        contexts.append(ctx)\n",
        "        if (i+1) % 10 == 0:\n",
        "            print(f\"Processed {i+1} rows...\")\n",
        "\n",
        "    df[\"Context\"] = contexts\n",
        "\n",
        "    # Save the output\n",
        "    print(f\"Saving to {OUTPUT_CSV}\")\n",
        "    df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
        "    print(\"Done!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading input CSV: C:/Users/nuref/Desktop/Stance_Detection_Datasets/VAST/VAST_dev.csv\n",
            "Processed 10 rows...\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 105\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 105\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[4], line 93\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     91\u001b[0m text_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     92\u001b[0m target_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m---> 93\u001b[0m ctx \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m contexts\u001b[38;5;241m.\u001b[39mappend(ctx)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "Cell \u001b[1;32mIn[4], line 58\u001b[0m, in \u001b[0;36mgenerate_context\u001b[1;34m(text, target)\u001b[0m\n\u001b[0;32m     52\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuthorization\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBearer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHF_API_TOKEN\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     55\u001b[0m }\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# POST request to your HF Endpoint\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mINFERENCE_ENDPOINT_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response\u001b[38;5;241m.\u001b[39mstatus_code, response\u001b[38;5;241m.\u001b[39mtext)\n",
            "File \u001b[1;32mc:\\Users\\nuref\\Desktop\\Stance_Detection_Datasets\\.venv\\Lib\\site-packages\\requests\\api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[1;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\nuref\\Desktop\\Stance_Detection_Datasets\\.venv\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\nuref\\Desktop\\Stance_Detection_Datasets\\.venv\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
            "File \u001b[1;32mc:\\Users\\nuref\\Desktop\\Stance_Detection_Datasets\\.venv\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
            "File \u001b[1;32mc:\\Users\\nuref\\Desktop\\Stance_Detection_Datasets\\.venv\\Lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
            "File \u001b[1;32mc:\\Users\\nuref\\Desktop\\Stance_Detection_Datasets\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\nuref\\Desktop\\Stance_Detection_Datasets\\.venv\\Lib\\site-packages\\urllib3\\connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
            "File \u001b[1;32mc:\\Users\\nuref\\Desktop\\Stance_Detection_Datasets\\.venv\\Lib\\site-packages\\urllib3\\connection.py:516\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    513\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    515\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    519\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
            "File \u001b[1;32mC:\\Python312\\Lib\\http\\client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1427\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1428\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1429\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
            "File \u001b[1;32mC:\\Python312\\Lib\\http\\client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[1;32mC:\\Python312\\Lib\\http\\client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mC:\\Python312\\Lib\\socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[1;32mC:\\Python312\\Lib\\ssl.py:1252\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1250\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1251\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
            "File \u001b[1;32mC:\\Python312\\Lib\\ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "############################\n",
        "# CONFIGURATION\n",
        "############################\n",
        "# Instead of local model paths, we have an endpoint URL:\n",
        "INFERENCE_ENDPOINT_URL = \"https://m8zx5wkcgrqynj2p.us-east-1.aws.endpoints.huggingface.cloud\"  # <-- your HF Inference Endpoint URL\n",
        "HF_API_TOKEN = \"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\"  # your HF token or endpoint token\n",
        "\n",
        "INPUT_CSV = \"C:/Users/nuref/Desktop/Stance_Detection_Datasets/VAST/VAST_dev.csv\"\n",
        "OUTPUT_CSV = \"C:/Users/nuref/Desktop/Stance_Detection_Datasets/VAST/VAST_dev_with_context.csv\"\n",
        "\n",
        "DATASET_DESCRIPTION = (\n",
        "    \"VAST isimli, 2020 yılında oluşturulmuş bu dataset, The New York Times’in ‘Room for Debate’ bölümündeki \"\n",
        "    \"yorumların etiketlenmesiyle hazırlanmış olup, özellikle tutum (stance) tespiti görevleri için tasarlanmıştır. \"\n",
        "    \"Politika (örneğin, 'Filistin devleti'), eğitim (örneğin, 'imtiyazlı okullar') ve halk sağlığı (örneğin, 'çocukluk aşıları') \"\n",
        "    \"gibi geniş temaları kapsayan çeşitli konuları kapsar. Ayrıca, ‘kampüste silahlar’ ile ‘kampüste ateşli silahlar’ \"\n",
        "    \"gibi benzer ifadelerin bir arada bulunduğu örnekleri de içerir.\"\n",
        ")\n",
        "\n",
        "############################\n",
        "# GENERATION FUNCTION\n",
        "############################\n",
        "def generate_context(text, target):\n",
        "    \"\"\"\n",
        "    Calls your remote Inference Endpoint with the prompt.\n",
        "    Expects the endpoint to return generated text in JSON format.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "Sosyal bir uzman olduğunu varsayarak, aşağıda {DATASET_DESCRIPTION} datasetinden\n",
        "kısa bir pasaj verilmiştir. Lütfen adım adım düşün, metindeki anahtar kelimeleri çıkar;\n",
        "yazarın ima ettiği duyguları, retorik araçları vb. analiz et;\n",
        "ve son olarak yazarın Target hakkındaki duruşuna dair kısa bir analiz yap, sonuca varmadan analiz sürecini vermeye dikkat et.\n",
        "Ayrıca açıklama 100 kelimeyi geçmesin ve hiçbir şekilde link içermesin.\n",
        "\n",
        "Passage: {text}\n",
        "Target: {target}\n",
        "\"\"\"\n",
        "\n",
        "    payload = {\n",
        "        \"inputs\": prompt,\n",
        "        \"parameters\": {\n",
        "            \"max_new_tokens\": 512,\n",
        "            \"temperature\": 0.7,\n",
        "            \"top_p\": 0.9,\n",
        "            \"do_sample\": True\n",
        "        }\n",
        "    }\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {HF_API_TOKEN}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    # POST request to your HF Endpoint\n",
        "    response = requests.post(INFERENCE_ENDPOINT_URL, headers=headers, json=payload)\n",
        "    if response.status_code != 200:\n",
        "        print(\"Error:\", response.status_code, response.text)\n",
        "        return \"\"  # return empty on error\n",
        "\n",
        "    data = response.json()\n",
        "    # Typically, data might be a list of dicts with \"generated_text\" or something similar\n",
        "    # e.g. [{\"generated_text\": \"...\"}]\n",
        "    if isinstance(data, list) and len(data) > 0 and \"generated_text\" in data[0]:\n",
        "        generated_text = data[0][\"generated_text\"]\n",
        "    elif \"generated_text\" in data:\n",
        "        generated_text = data[\"generated_text\"]\n",
        "    else:\n",
        "        generated_text = \"\"\n",
        "\n",
        "    # Optionally truncate to 100 words\n",
        "    words = generated_text.split()\n",
        "    if len(words) > 100:\n",
        "        generated_text = \" \".join(words[:100])\n",
        "    return generated_text.strip()\n",
        "\n",
        "############################\n",
        "# MAIN FLOW\n",
        "############################\n",
        "def main():\n",
        "    print(f\"Reading input CSV: {INPUT_CSV}\")\n",
        "    df = pd.read_csv(INPUT_CSV, encoding=\"utf-8\")\n",
        "\n",
        "    if \"Text\" not in df.columns or \"Target\" not in df.columns:\n",
        "        raise ValueError(\"Input CSV must have 'Text' and 'Target' columns.\")\n",
        "\n",
        "    contexts = []\n",
        "    for i, row in df.iterrows():\n",
        "        text_val = str(row[\"Text\"])\n",
        "        target_val = str(row[\"Target\"])\n",
        "        ctx = generate_context(text_val, target_val)\n",
        "        contexts.append(ctx)\n",
        "        if (i+1) % 10 == 0:\n",
        "            print(f\"Processed {i+1} rows...\")\n",
        "\n",
        "    df[\"Context\"] = contexts\n",
        "\n",
        "    print(f\"Saving to {OUTPUT_CSV}\")\n",
        "    df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
        "    print(\"Done!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0a85adfed4df4ccba0c03d6b73731b2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9b3f1bed13034a3cba856f329ad22d9f",
              "IPY_MODEL_b03468753cff49ec8ed41a092ff15611",
              "IPY_MODEL_80ded8de1c774168b9d2c40d4797af28"
            ],
            "layout": "IPY_MODEL_e83735d01e47463f8ed60f6f68bfaf4b"
          }
        },
        "147284d33327479ba769c224012f5bde": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "18c3b7d9d31c47f19f5015a5e3beb30d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4ea4c6c289be4f308c18d337f7126ebd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "561785e53e41408596ebd8a82a23d05a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6495ce6380474d2eaabbee6df2586e7a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80ded8de1c774168b9d2c40d4797af28": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6495ce6380474d2eaabbee6df2586e7a",
            "placeholder": "​",
            "style": "IPY_MODEL_147284d33327479ba769c224012f5bde",
            "value": " 4/4 [01:19&lt;00:00, 26.41s/it]"
          }
        },
        "9b3f1bed13034a3cba856f329ad22d9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a24c613e62f2495eb45a11c2b61a92f7",
            "placeholder": "​",
            "style": "IPY_MODEL_4ea4c6c289be4f308c18d337f7126ebd",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "a24c613e62f2495eb45a11c2b61a92f7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b03468753cff49ec8ed41a092ff15611": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_561785e53e41408596ebd8a82a23d05a",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_18c3b7d9d31c47f19f5015a5e3beb30d",
            "value": 4
          }
        },
        "e83735d01e47463f8ed60f6f68bfaf4b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

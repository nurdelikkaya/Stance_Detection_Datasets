{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNUF+3NZjpS1fy1VwTBrvnN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["pip install chardet"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I7X4i_h6I2fb","executionInfo":{"status":"ok","timestamp":1735554992756,"user_tz":-180,"elapsed":3589,"user":{"displayName":"Nur Efşan Delikkaya (Student)","userId":"07973742884581158941"}},"outputId":"765e5241-6120-42f1-9daa-bc89b5f360aa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (5.2.0)\n"]}]},{"cell_type":"code","source":["import chardet\n","\n","with open(\"/content/text_translated.csv\", \"rb\") as f:\n","    raw_data = f.read()\n","    detect_result = chardet.detect(raw_data)\n","\n","print(detect_result)\n","# Example output: {'encoding': 'ISO-8859-9', 'confidence': 0.99, 'language': ''}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OqAVN5rRI6Hp","executionInfo":{"status":"ok","timestamp":1735555009298,"user_tz":-180,"elapsed":4278,"user":{"displayName":"Nur Efşan Delikkaya (Student)","userId":"07973742884581158941"}},"outputId":"af81f7c1-2570-4c78-ef8b-37a906c26c0f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'encoding': 'Windows-1252', 'confidence': 0.7290234569437531, 'language': ''}\n"]}]},{"cell_type":"code","source":["text_df = pd.read_excel(\"/content/text_translated.xlsx\", dtype=str)\n","print(text_df.columns)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ATAEsXWjlx6B","executionInfo":{"status":"ok","timestamp":1735579357977,"user_tz":-180,"elapsed":777,"user":{"displayName":"Nur Efşan Delikkaya (Student)","userId":"07973742884581158941"}},"outputId":"6882e467-4e50-43d0-b44b-ddae23210e7b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Index(['İD', ' metin'], dtype='object')\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"nUinAOoTHfRA","executionInfo":{"status":"error","timestamp":1735644047086,"user_tz":-180,"elapsed":279,"user":{"displayName":"Nur Efşan Delikkaya (Student)","userId":"07973742884581158941"}},"outputId":"52c740e0-8389-44f2-a36a-1c8018c5f17d"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/text_translated.xlsx'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-9a788ae4208e>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 1. Read the Excel file into a pandas DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/text_translated.xlsx\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# or \"latin-1\" etc.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/text_translated.xlsx'"]}],"source":["import os\n","import json\n","import pandas as pd\n","\n","# 1. Read the Excel file into a pandas DataFrame\n","text_df = pd.read_excel(\"/content/text_biden.xlsx\", dtype=str)\n","# dtype=str helps ensure 'kimlik' like '1-1' isn't auto-converted\n","\n","# 2. Create a lookup dict: kimlik -> metin\n","kimlik_to_metin = dict(zip(text_df['İD'], text_df[' metin']))\n","\n","# 3. Define stance mapping\n","stance_map = {\n","    \"none\": 0,\n","    \"favor\": 1,\n","    \"against\": -1\n","}\n","\n","# 4. JSON files (train, valid, test)\n","json_files = [\n","    \"/content/train_biden.json\",\n","    \"/content/valid_biden.json\",\n","    \"/content/test_biden.json\"\n","]\n","\n","# A list to store final rows for the new CSV\n","final_data = []\n","\n","for json_file in json_files:\n","    # Read the JSON file\n","    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n","        json_array = json.load(f)  # Each file is presumably a list of objects\n","\n","    for obj in json_array:\n","        stance_str = obj[\"stance\"]  # \"favor\", \"none\", \"against\"\n","        stance_numeric = stance_map.get(stance_str, 0)\n","\n","        # Approach #1: ONLY take the LAST item from obj[\"index\"]\n","        if obj[\"index\"]:  # make sure the list isn't empty\n","            last_idx = obj[\"index\"][-1]\n","            # If it's in our dictionary, retrieve text\n","            if last_idx in kimlik_to_metin:\n","                text_value = kimlik_to_metin[last_idx]\n","                # Append [Target, Stance, Text] to final_data\n","                final_data.append([\"Biden\", stance_numeric, text_value])\n","            # else:\n","            #     print(f\"WARN: kimlik {last_idx} not found in text.xlsx\")\n","\n","# 6. Convert final_data into a DataFrame\n","df_output = pd.DataFrame(final_data, columns=[\"Target\", \"Stance\", \"Text\"])\n","\n","# 7. Save to a new CSV file\n","output_csv = \"biden_stance.csv\"\n","df_output.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n","\n","print(f\"Done! Created {output_csv} with {len(df_output)} rows.\")\n"]},{"cell_type":"code","source":["import os\n","import json\n","import pandas as pd\n","\n","# 1. Read the Excel file into a pandas DataFrame\n","text_df = pd.read_excel(\"/content/text_bitcoin.xlsx\", dtype=str)\n","# dtype=str helps ensure 'kimlik' like '1-1' isn't auto-converted\n","\n","# 2. Create a lookup dict: kimlik -> metin\n","kimlik_to_metin = dict(zip(text_df['İD'], text_df[' metin']))\n","\n","# 3. Define stance mapping\n","stance_map = {\n","    \"none\": 0,\n","    \"favor\": 1,\n","    \"against\": -1\n","}\n","\n","# 4. JSON files (train, valid, test)\n","json_files = [\n","    \"/content/train_bitcoin.json\",\n","    \"/content/valid_bitcoin.json\",\n","    \"/content/test_bitcoin.json\"\n","]\n","\n","# A list to store final rows for the new CSV\n","final_data = []\n","\n","for json_file in json_files:\n","    # Read the JSON file\n","    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n","        json_array = json.load(f)  # Each file is presumably a list of objects\n","\n","    for obj in json_array:\n","        stance_str = obj[\"stance\"]  # \"favor\", \"none\", \"against\"\n","        stance_numeric = stance_map.get(stance_str, 0)\n","\n","        # Approach #1: ONLY take the LAST item from obj[\"index\"]\n","        if obj[\"index\"]:  # make sure the list isn't empty\n","            last_idx = obj[\"index\"][-1]\n","            # If it's in our dictionary, retrieve text\n","            if last_idx in kimlik_to_metin:\n","                text_value = kimlik_to_metin[last_idx]\n","                # Append [Target, Stance, Text] to final_data\n","                final_data.append([\"Bitcoin\", stance_numeric, text_value])\n","            # else:\n","            #     print(f\"WARN: kimlik {last_idx} not found in text.xlsx\")\n","\n","# 6. Convert final_data into a DataFrame\n","df_output = pd.DataFrame(final_data, columns=[\"Target\", \"Stance\", \"Text\"])\n","\n","# 7. Save to a new CSV file\n","output_csv = \"bitcoin_stance.csv\"\n","df_output.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n","\n","print(f\"Done! Created {output_csv} with {len(df_output)} rows.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ycDYe2ylr3c7","executionInfo":{"status":"ok","timestamp":1735624514744,"user_tz":-180,"elapsed":425,"user":{"displayName":"Nur Efşan Delikkaya (Student)","userId":"07973742884581158941"}},"outputId":"93b0e404-699e-4e1d-ed86-7915f9e191c5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Done! Created bitcoin_stance.csv with 111 rows.\n"]}]},{"cell_type":"code","source":["import os\n","import json\n","import pandas as pd\n","\n","# 1. Read the Excel file into a pandas DataFrame\n","text_df = pd.read_excel(\"/content/text_spaceX.xlsx\", dtype=str)\n","# dtype=str helps ensure 'kimlik' like '1-1' isn't auto-converted\n","\n","# 2. Create a lookup dict: kimlik -> metin\n","kimlik_to_metin = dict(zip(text_df['İD'], text_df[' metin']))\n","\n","# 3. Define stance mapping\n","stance_map = {\n","    \"none\": 0,\n","    \"favor\": 1,\n","    \"against\": -1\n","}\n","\n","# 4. JSON files (train, valid, test)\n","json_files = [\n","    \"/content/train_spaceX.json\",\n","    \"/content/valid_spaceX.json\",\n","    \"/content/test_spaceX.json\"\n","]\n","\n","# A list to store final rows for the new CSV\n","final_data = []\n","\n","for json_file in json_files:\n","    # Read the JSON file\n","    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n","        json_array = json.load(f)  # Each file is presumably a list of objects\n","\n","    for obj in json_array:\n","        stance_str = obj[\"stance\"]  # \"favor\", \"none\", \"against\"\n","        stance_numeric = stance_map.get(stance_str, 0)\n","\n","        # Approach #1: ONLY take the LAST item from obj[\"index\"]\n","        if obj[\"index\"]:  # make sure the list isn't empty\n","            last_idx = obj[\"index\"][-1]\n","            # If it's in our dictionary, retrieve text\n","            if last_idx in kimlik_to_metin:\n","                text_value = kimlik_to_metin[last_idx]\n","                # Append [Target, Stance, Text] to final_data\n","                final_data.append([\"SpaceX\", stance_numeric, text_value])\n","            # else:\n","            #     print(f\"WARN: kimlik {last_idx} not found in text.xlsx\")\n","\n","# 6. Convert final_data into a DataFrame\n","df_output = pd.DataFrame(final_data, columns=[\"Target\", \"Stance\", \"Text\"])\n","\n","# 7. Save to a new CSV file\n","output_csv = \"spaceX_stance.csv\"\n","df_output.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n","\n","print(f\"Done! Created {output_csv} with {len(df_output)} rows.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HGcDYDYPRz9z","executionInfo":{"status":"ok","timestamp":1735624564748,"user_tz":-180,"elapsed":211,"user":{"displayName":"Nur Efşan Delikkaya (Student)","userId":"07973742884581158941"}},"outputId":"5a79d8e6-792b-471a-de17-3efe370786c4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Done! Created spaceX_stance.csv with 65 rows.\n"]}]},{"cell_type":"code","source":["import os\n","import json\n","import pandas as pd\n","\n","# 1. Read the Excel file into a pandas DataFrame\n","text_df = pd.read_excel(\"/content/text_tesla.xlsx\", dtype=str)\n","# dtype=str helps ensure 'kimlik' like '1-1' isn't auto-converted\n","\n","# 2. Create a lookup dict: kimlik -> metin\n","kimlik_to_metin = dict(zip(text_df['İD'], text_df[' metin']))\n","\n","# 3. Define stance mapping\n","stance_map = {\n","    \"none\": 0,\n","    \"favor\": 1,\n","    \"against\": -1\n","}\n","\n","# 4. JSON files (train, valid, test)\n","json_files = [\n","    \"/content/train_tesla.json\",\n","    \"/content/valid_tesla.json\",\n","    \"/content/test_tesla.json\"\n","]\n","\n","# A list to store final rows for the new CSV\n","final_data = []\n","\n","for json_file in json_files:\n","    # Read the JSON file\n","    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n","        json_array = json.load(f)  # Each file is presumably a list of objects\n","\n","    for obj in json_array:\n","        stance_str = obj[\"stance\"]  # \"favor\", \"none\", \"against\"\n","        stance_numeric = stance_map.get(stance_str, 0)\n","\n","        # Approach #1: ONLY take the LAST item from obj[\"index\"]\n","        if obj[\"index\"]:  # make sure the list isn't empty\n","            last_idx = obj[\"index\"][-1]\n","            # If it's in our dictionary, retrieve text\n","            if last_idx in kimlik_to_metin:\n","                text_value = kimlik_to_metin[last_idx]\n","                # Append [Target, Stance, Text] to final_data\n","                final_data.append([\"Tesla\", stance_numeric, text_value])\n","            # else:\n","            #     print(f\"WARN: kimlik {last_idx} not found in text.xlsx\")\n","\n","# 6. Convert final_data into a DataFrame\n","df_output = pd.DataFrame(final_data, columns=[\"Target\", \"Stance\", \"Text\"])\n","\n","# 7. Save to a new CSV file\n","output_csv = \"tesla_stance.csv\"\n","df_output.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n","\n","print(f\"Done! Created {output_csv} with {len(df_output)} rows.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ElRYPFEOTEBc","executionInfo":{"status":"ok","timestamp":1735624816640,"user_tz":-180,"elapsed":477,"user":{"displayName":"Nur Efşan Delikkaya (Student)","userId":"07973742884581158941"}},"outputId":"a5913e3b-885d-4710-f945-800b46785fff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Done! Created tesla_stance.csv with 124 rows.\n"]}]},{"cell_type":"code","source":["import os\n","import json\n","import pandas as pd\n","\n","# 1. Read the Excel file into a pandas DataFrame\n","text_df = pd.read_excel(\"/content/text_trump.xlsx\", dtype=str)\n","# dtype=str helps ensure 'kimlik' like '1-1' isn't auto-converted\n","\n","# 2. Create a lookup dict: kimlik -> metin\n","kimlik_to_metin = dict(zip(text_df['İD'], text_df[' metin']))\n","\n","# 3. Define stance mapping\n","stance_map = {\n","    \"none\": 0,\n","    \"favor\": 1,\n","    \"against\": -1\n","}\n","\n","# 4. JSON files (train, valid, test)\n","json_files = [\n","    \"/content/train_trump.json\",\n","    \"/content/valid_trump.json\",\n","    \"/content/test_trump.json\"\n","]\n","\n","# A list to store final rows for the new CSV\n","final_data = []\n","\n","for json_file in json_files:\n","    # Read the JSON file\n","    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n","        json_array = json.load(f)  # Each file is presumably a list of objects\n","\n","    for obj in json_array:\n","        stance_str = obj[\"stance\"]  # \"favor\", \"none\", \"against\"\n","        stance_numeric = stance_map.get(stance_str, 0)\n","\n","        # Approach #1: ONLY take the LAST item from obj[\"index\"]\n","        if obj[\"index\"]:  # make sure the list isn't empty\n","            last_idx = obj[\"index\"][-1]\n","            # If it's in our dictionary, retrieve text\n","            if last_idx in kimlik_to_metin:\n","                text_value = kimlik_to_metin[last_idx]\n","                # Append [Target, Stance, Text] to final_data\n","                final_data.append([\"Trump\", stance_numeric, text_value])\n","            # else:\n","            #     print(f\"WARN: kimlik {last_idx} not found in text.xlsx\")\n","\n","# 6. Convert final_data into a DataFrame\n","df_output = pd.DataFrame(final_data, columns=[\"Target\", \"Stance\", \"Text\"])\n","\n","# 7. Save to a new CSV file\n","output_csv = \"trump_stance.csv\"\n","df_output.to_csv(output_csv, index=False, encoding=\"utf-8-sig\")\n","\n","print(f\"Done! Created {output_csv} with {len(df_output)} rows.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ArjngF15PPAj","executionInfo":{"status":"ok","timestamp":1735640811009,"user_tz":-180,"elapsed":1438,"user":{"displayName":"Nur Efşan Delikkaya (Student)","userId":"07973742884581158941"}},"outputId":"fd45f8f4-e841-4e8a-a87a-34fe533dd7a7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Done! Created trump_stance.csv with 132 rows.\n"]}]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","# 1. List of your CSV input files\n","csv_files = [\n","    \"/content/biden_stance.csv\",\n","    \"/content/bitcoin_stance.csv\",\n","    \"/content/spaceX_stance.csv\",\n","    \"/content/tesla_stance.csv\",\n","    \"/content/trump_stance.csv\"\n","]\n","\n","# 2. DataFrames for cumulative splits\n","all_train_dfs = []\n","all_test_dfs = []\n","all_dev_dfs = []\n","\n","for csv_path in csv_files:\n","    # Read each CSV\n","    df = pd.read_csv(csv_path, encoding=\"utf-8\")\n","\n","    # Shuffle & split -> 70% train, 30% temp\n","    train_df, temp_df = train_test_split(\n","        df,\n","        test_size=0.30,\n","        random_state=42,  # for reproducibility\n","        shuffle=True\n","    )\n","\n","    # Split temp -> 50% test, 50% dev (so each is 15% of the original CSV)\n","    test_df, dev_df = train_test_split(\n","        temp_df,\n","        test_size=0.50,\n","        random_state=42,\n","        shuffle=True\n","    )\n","\n","    # Append partial splits\n","    all_train_dfs.append(train_df)\n","    all_test_dfs.append(test_df)\n","    all_dev_dfs.append(dev_df)\n","\n","# 3. Combine splits from all CSVs into final dataframes\n","combined_train_df = pd.concat(all_train_dfs, ignore_index=True)\n","combined_test_df = pd.concat(all_test_dfs, ignore_index=True)\n","combined_dev_df = pd.concat(all_dev_dfs, ignore_index=True)\n","\n","# 4. Save final splits\n","combined_train_df.to_csv(\"MT-CSD_train.csv\", index=False, encoding=\"utf-8-sig\")\n","combined_test_df.to_csv(\"MT-CSD_test.csv\", index=False, encoding=\"utf-8-sig\")\n","combined_dev_df.to_csv(\"MT-CSD_dev.csv\", index=False, encoding=\"utf-8-sig\")\n","\n","print(\"Combined splits created:\")\n","print(f\"Train: {len(combined_train_df)} rows\")\n","print(f\"Test:  {len(combined_test_df)} rows\")\n","print(f\"Dev:   {len(combined_dev_df)} rows\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PwVYE7BcI4r3","executionInfo":{"status":"ok","timestamp":1735722886570,"user_tz":-180,"elapsed":2872,"user":{"displayName":"Nur Efşan Delikkaya (Student)","userId":"07973742884581158941"}},"outputId":"baec8a82-d2ba-48ba-ea8e-60315db7562c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Combined splits created:\n","Train: 362 rows\n","Test:  79 rows\n","Dev:   80 rows\n"]}]}]}